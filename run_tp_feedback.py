#!/usr/bin/env python3
"""
Multi-Round Feedback Inference Script (Simplified)
ç®€åŒ–çš„å¤šè½®åé¦ˆæ¨ç†è„šæœ¬ - åªè´Ÿè´£éªŒè¯å’Œæ¨ç†é€»è¾‘

This script handles inference and validation logic only.
Server deployment is handled by the bash script.
"""

import os
import sys
import subprocess
import time
import json
import requests
import argparse
from pathlib import Path

class MultiRoundInference:
    def __init__(self, model_name, dataset_name, split_num, prompt_config, max_rounds):
        # åŸºæœ¬å‚æ•°
        self.model_name = model_name
        self.dataset_name = dataset_name
        self.split_num = f"{int(split_num):02d}"
        self.prompt_config = prompt_config
        self.max_rounds = int(max_rounds)
        
        # è·¯å¾„é…ç½®
        self.oprover_dir = Path("/data/code/Oprover")
        self.data_root_dir = Path("/madehua/data/oprover/generated_data")
        self.validation_service_dir = Path("/data/code/kimina-lean-server/server/proof")
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.output_dir = self.data_root_dir / self.dataset_name / f"{self.model_name}_results"
        self.validation_dir = self.output_dir  # éªŒè¯ç»“æœä¹Ÿæ”¾åœ¨æ¨¡å‹ç‰¹å®šç›®å½•ä¸‹
        self.log_dir = self.output_dir / "logs"  # æ—¥å¿—æ”¾åœ¨æ¨¡å‹ç‰¹å®šç›®å½•çš„logså­ç›®å½•ä¸‹
        
        for dir_path in [self.output_dir, self.log_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # éªŒè¯æœåŠ¡é…ç½®
        self.validation_server_port = 8002
        self.validation_script = self.validation_service_dir / "lean_proof_single.py"
        
        # æ¨ç†é…ç½®
        self.config_file = self.get_model_config()
        self.log_prefix = self.get_log_prefix()
        self.batch_size = "3000"
        
        # åˆå§‹æ•°æ®é›†è·¯å¾„
        self.initial_split = f"{self.dataset_name}/lean_statement_part_{self.split_num}"
    
    def get_model_config(self):
        """æ ¹æ®æ¨¡å‹åç§°è¿”å›é…ç½®æ–‡ä»¶è·¯å¾„"""
        config_map = {
            "DeepSeek-Prover-V2-7B": "config/config_dpsk.yaml",
            "Goedel-Prover-V2-8B": "config/config_goedel.yaml", 
            "Goedel-Prover-V2-32B": "config/config_goedel.yaml",
            "Kimina-Prover-72B": "config/config_kimina.yaml"
        }
        return config_map.get(self.model_name, "config/config_dpsk.yaml")
    
    def get_log_prefix(self):
        """æ ¹æ®æ¨¡å‹åç§°è¿”å›æ—¥å¿—å‰ç¼€"""
        prefix_map = {
            "DeepSeek-Prover-V2-7B": "dpsk",
            "Goedel-Prover-V2-8B": "g8",
            "Goedel-Prover-V2-32B": "g32", 
            "Kimina-Prover-72B": "k72"
        }
        if self.model_name in prefix_map:
            return prefix_map[self.model_name]
        else:
            import re
            simplified = re.sub(r'[^a-z0-9]', '', self.model_name.lower())
            return simplified[:10]
    
    def check_service_health(self):
        """æ£€æŸ¥éªŒè¯æœåŠ¡æ˜¯å¦è¿è¡Œ"""
        try:
            response = requests.get(f"http://localhost:{self.validation_server_port}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def run_inference_round(self, round_num, input_split, round_output_dir):
        """è¿è¡Œå•è½®æ¨ç†"""
        print(f"=== Round {round_num}: Starting inference ===")
        print(f"Input split: {input_split}")
        print(f"Output dir: {round_output_dir}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(round_output_dir).mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºæ¨ç†å‘½ä»¤
        log_file = self.log_dir / f"{self.log_prefix}_{self.prompt_config}_round{round_num}_worker_{self.split_num}.log"
        
        cmd = [
            "python", "infer/infer.py",
            "--config", self.config_file,
            "--split", input_split,
            "--mode", f"{self.prompt_config}-bon",
            "--model_name", self.model_name,
            "--output_dir", round_output_dir,
            "--batch_size", self.batch_size,
            "--use_accel",
            "--index", "0", 
            "--world_size", "1"
        ]
        
        try:
            print(f"Running command: {' '.join(cmd)}")
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            env['PYTHONPATH'] = str(self.oprover_dir)
            
            # ä¸ºdata_loader.pyè®¾ç½®è¾“å…¥æ–‡ä»¶è·¯å¾„
            if round_num == 1:
                # ç¬¬1è½®ä½¿ç”¨åŸå§‹æ•°æ®
                env['INPUT_FILE'] = str(self.initial_split)
            else:
                # åç»­è½®æ¬¡ä½¿ç”¨ä¸Šä¸€è½®çš„éªŒè¯ç»“æœ
                prev_validation_file = self.validation_dir / f"part_{self.split_num}_round{round_num-1}_proof.jsonl"
                env['INPUT_FILE'] = str(prev_validation_file)
            
            # è®¾ç½®è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œè®©infer.pyç›´æ¥è¾“å‡ºåˆ°æœŸæœ›çš„ä½ç½®
            inference_output_file = self.output_dir / f"part_{self.split_num}_round{round_num}_inference.jsonl"
            env['OUTPUT_FILE'] = str(inference_output_file)
            
            # è¿è¡Œæ¨ç†
            with open(log_file, 'w') as f:
                process = subprocess.run(cmd, stdout=f, stderr=subprocess.STDOUT, env=env, cwd=self.oprover_dir)
            
            # ä¸ä¾èµ–è¿”å›ç ï¼Œè€Œæ˜¯æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç›®æ ‡æ–‡ä»¶
            # ç°åœ¨infer.pyä¼šç›´æ¥è¾“å‡ºåˆ°æœŸæœ›çš„è·¯å¾„
            expected_output_file = self.output_dir / f"part_{self.split_num}_round{round_num}_inference.jsonl"
            
            if expected_output_file.exists():
                print(f"âœ… Round {round_num} inference completed successfully (output file found)")
                return True
            else:
                print(f"âŒ Round {round_num} inference failed (no output file generated)")
                print(f"Expected output file: {expected_output_file}")
                return False
            
        except Exception as e:
            print(f"âŒ Error in round {round_num} inference: {e}")
            return False
    
    def run_validation(self, round_num, inference_output_file, validation_output_file):
        """è¿è¡ŒéªŒè¯"""
        print(f"=== Round {round_num}: Starting validation ===")
        print(f"Inference output: {inference_output_file}")
        print(f"Target validation output: {validation_output_file}")
        
        # æ£€æŸ¥éªŒè¯æœåŠ¡æ˜¯å¦è¿è¡Œ
        if not self.check_service_health():
            print("âŒ Validation service is not running")
            return False
        
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            env['INPUT_FILE'] = str(inference_output_file)
            
            # è¿è¡ŒéªŒè¯è„šæœ¬ï¼Œä¼ é€’å®Œæ•´çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
            log_file = self.log_dir / f"validation_round{round_num}_{self.split_num}.log"
            
            cmd = [
                "python", str(self.validation_script),
                "--output_file", str(validation_output_file)
            ]
            
            print(f"Running validation command: {' '.join(cmd)}")
            
            with open(log_file, 'w') as f:
                process = subprocess.run(cmd, stdout=f, stderr=subprocess.STDOUT, env=env, cwd=self.validation_service_dir)
            
            if process.returncode == 0:
                # éªŒè¯è„šæœ¬åº”è¯¥ç›´æ¥è¾“å‡ºåˆ°æŒ‡å®šæ–‡ä»¶è·¯å¾„
                if Path(validation_output_file).exists():
                    print(f"âœ… Round {round_num} validation completed successfully")
                    print(f"Validation results saved to: {validation_output_file}")
                else:
                    print(f"âŒ Validation output file not found at: {validation_output_file}")
                    return False
            else:
                print(f"âŒ Round {round_num} validation failed with return code: {process.returncode}")
            
            return process.returncode == 0
            
        except Exception as e:
            print(f"âŒ Error in round {round_num} validation: {e}")
            return False
    
    def check_validation_results(self, validation_file, round_num):
        """æ£€æŸ¥éªŒè¯ç»“æœï¼Œè¿”å›å¤±è´¥æ¡ˆä¾‹æ•°"""
        print(f"=== Round {round_num} Validation Results ===")
        
        try:
            failed_count = 0
            success_count = 0
            total_count = 0
            
            with open(validation_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        total_count += 1
                        if data.get('success', False):
                            success_count += 1
                        else:
                            failed_count += 1
                    except json.JSONDecodeError:
                        continue
            
            # è®¡ç®—æ­£ç¡®ç‡
            if total_count > 0:
                accuracy = (success_count / total_count) * 100
                print(f"ğŸ“Š Round {round_num} Statistics:")
                print(f"   âœ… Successful: {success_count}")
                print(f"   âŒ Failed: {failed_count}")
                print(f"   ğŸ“ˆ Total: {total_count}")
                print(f"   ğŸ¯ Accuracy: {accuracy:.2f}%")
            else:
                print(f"âš ï¸  No validation results found in round {round_num}")
            
            print("=" * 40)
            return failed_count
            
        except Exception as e:
            print(f"âŒ Error checking validation results: {e}")
            return 0
    
    def run(self):
        """è¿è¡Œå¤šè½®æ¨ç†ä¸»å¾ªç¯"""
        print("Starting multi-round inference process...")
        print(f"Model: {self.model_name}")
        print(f"Dataset: {self.dataset_name}")
        print(f"Split: {self.split_num}")
        print(f"Prompt Config: {self.prompt_config}")
        print(f"Max Rounds: {self.max_rounds}")
        print("=" * 50)
        
        current_split = self.initial_split
        
        for round_num in range(1, self.max_rounds + 1):
            print()
            print("/" * 50)
            print(f"/// ROUND {round_num} / {self.max_rounds}")
            print("/" * 50)
            
            # è®¾ç½®å½“å‰è½®æ¬¡çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
            # ç°åœ¨infer.pyä¼šç›´æ¥è¾“å‡ºåˆ°æœŸæœ›çš„è·¯å¾„
            inference_output_file = self.output_dir / f"part_{self.split_num}_round{round_num}_inference.jsonl"
            # ç®€åŒ–çš„éªŒè¯æ–‡ä»¶åï¼ˆåŒ…å«è½®æ¬¡ï¼‰
            validation_output_file = self.validation_dir / f"part_{self.split_num}_round{round_num}_proof.jsonl"
            
            # ç¬¬1æ­¥ï¼šè¿è¡Œæ¨ç†
            print(f"Starting round {round_num} inference...")
            
            try:
                self.run_inference_round(round_num, current_split, str(self.output_dir))
            except Exception as e:
                print(f"âš ï¸  Round {round_num} inference encountered an error: {e}")
                print(f"â­ï¸  Continuing to check if output file was generated...")
            
            # æ£€æŸ¥æ¨ç†è„šæœ¬ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶
            if not inference_output_file.exists():
                print(f"âŒ Round {round_num} failed: No inference output file generated")
                print(f"Expected file: {inference_output_file}")
                print(f"â­ï¸  Skipping round {round_num} and continuing...")
                return False
            
            print(f"âœ… Round {round_num} inference result saved: {inference_output_file.name}")
            
            # ç¬¬2æ­¥ï¼šè¿è¡ŒéªŒè¯
            if not self.run_validation(round_num, str(inference_output_file), str(validation_output_file)):
                print(f"âŒ Round {round_num} validation failed. Stopping.")
                return False
            
            # æ£€æŸ¥éªŒè¯è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not validation_output_file.exists():
                print(f"âŒ Validation output file not found: {validation_output_file}")
                return False
            
            # ç¬¬3æ­¥ï¼šæ£€æŸ¥éªŒè¯ç»“æœ
            failed_count = self.check_validation_results(str(validation_output_file), round_num)
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€è½®ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­
            if round_num < self.max_rounds:
                if failed_count == 0:
                    print("ğŸ‰ All problems solved successfully! No need for further rounds.")
                    break
                
                # è®¾ç½®ä¸‹ä¸€è½®çš„è¾“å…¥ï¼ˆä½¿ç”¨ç±»ä¼¼ FineLeanCorpus/lean_statement_part_00 çš„splitæ ¼å¼ï¼‰
                # data_loader.pyä¼šè‡ªåŠ¨è¿‡æ»¤æ‰æˆåŠŸçš„æ¡ˆä¾‹
                current_split = f"{self.dataset_name}/lean_statement_part_{self.split_num}"
                print(f"â¡ï¸  Next round will use: split={current_split}")
                print(f"   (Input file: {validation_output_file})")
        
        print()
        print("/" * 50)
        print("/// MULTI-ROUND PROCESS COMPLETED")
        print("/" * 50)
        
        # æœ€ç»ˆæ±‡æ€»ç»Ÿè®¡
        print("ğŸ“Š FINAL RESULTS SUMMARY")
        print("=" * 50)
        
        all_rounds_stats = []
        for round_num in range(1, self.max_rounds + 1):
            validation_file = self.validation_dir / f"part_{self.split_num}_round{round_num}_proof.jsonl"
            if validation_file.exists():
                try:
                    success_count = 0
                    failed_count = 0
                    total_count = 0
                    with open(validation_file, 'r') as f:
                        for line in f:
                            try:
                                data = json.loads(line.strip())
                                total_count += 1
                                if data.get('success', False):
                                    success_count += 1
                                else:
                                    failed_count += 1
                            except json.JSONDecodeError:
                                continue
                    
                    if total_count > 0:
                        accuracy = (success_count / total_count) * 100
                        all_rounds_stats.append((round_num, success_count, failed_count, total_count, accuracy))
                        print(f"Round {round_num}: {success_count}/{total_count} successful ({accuracy:.1f}%)")
                    else:
                        print(f"Round {round_num}: No results")
                except Exception as e:
                    print(f"Round {round_num}: Error reading results - {e}")
        
        print("=" * 50)
        
        print()
        print(f"All output files (inference + validation) saved in: {self.output_dir}")
        print(f"All logs saved in: {self.log_dir}")
        print("âœ… Multi-round feedback process completed successfully!")
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Multi-Round Feedback Inference Script')
    parser.add_argument('--model_name', type=str, default='DeepSeek-Prover-V2-7B', help='Model name')
    parser.add_argument('--dataset_name', type=str, default='FineLeanCorpus', help='Dataset name')
    parser.add_argument('--split_num', type=str, default='00', help='Split number')
    parser.add_argument('--prompt_config', type=str, default='proof_cot_feedback', help='Prompt configuration')
    parser.add_argument('--max_rounds', type=str, default='3', help='Maximum number of rounds')
    
    args = parser.parse_args()
    
    try:
        runner = MultiRoundInference(
            args.model_name, 
            args.dataset_name, 
            args.split_num, 
            args.prompt_config, 
            args.max_rounds
        )
        success = runner.run()
        return 0 if success else 1
    except KeyboardInterrupt:
        print("\nInterrupted by user.")
        return 1
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())